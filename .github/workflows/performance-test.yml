name: ğŸš€ Performance Tests

on:
  schedule:
    # Ejecutar tests de rendimiento diariamente a las 6:00 UTC
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Permitir ejecuciÃ³n manual

env:
  NODEJS_BACKEND_URL: https://y0h0i3c86qv6.manus.space
  N8N_BACKEND_URL: https://77h9ikc6nzl1.manus.space

jobs:
  performance-test:
    name: ğŸ“Š Performance & Load Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v4
      
    - name: ğŸ Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests concurrent.futures statistics
        
    - name: ğŸš€ Run Performance Tests
      run: |
        python -c "
        import requests
        import time
        import statistics
        import concurrent.futures
        import json
        
        NODEJS_URL = '${{ env.NODEJS_BACKEND_URL }}'
        N8N_URL = '${{ env.N8N_BACKEND_URL }}'
        
        def test_endpoint(backend_name, backend_url, query, user_id):
            payload = {
                'userId': f'perf_test_{user_id}',
                'message': query,
                'isSubscriber': False
            }
            
            start_time = time.time()
            try:
                response = requests.post(f'{backend_url}/chat', 
                                       json=payload, timeout=60)
                end_time = time.time()
                
                if response.status_code == 200:
                    data = response.json()
                    traces = data.get('trace', data.get('traces', []))
                    return {
                        'backend': backend_name,
                        'success': True,
                        'response_time': end_time - start_time,
                        'trace_count': len(traces),
                        'query': query
                    }
                else:
                    return {
                        'backend': backend_name,
                        'success': False,
                        'response_time': end_time - start_time,
                        'error': f'HTTP {response.status_code}',
                        'query': query
                    }
            except Exception as e:
                return {
                    'backend': backend_name,
                    'success': False,
                    'response_time': time.time() - start_time,
                    'error': str(e),
                    'query': query
                }
        
        # Test queries
        test_queries = ['10000', '10090', 'Â¿CuÃ¡l es el horario?', 'informaciÃ³n']
        backends = [('Node.js', NODEJS_URL), ('N8N', N8N_URL)]
        
        print('ğŸš€ Starting performance tests...')
        print('=' * 50)
        
        all_results = []
        
        # Sequential tests
        for backend_name, backend_url in backends:
            print(f'Testing {backend_name} backend...')
            
            for query in test_queries:
                result = test_endpoint(backend_name, backend_url, query, 'sequential')
                all_results.append(result)
                
                if result['success']:
                    print(f'  âœ… {query}: {result[\"response_time\"]:.2f}s, {result[\"trace_count\"]} traces')
                else:
                    print(f'  âŒ {query}: {result[\"error\"]}')
        
        # Concurrent tests (light load)
        print('\\nğŸ”„ Running concurrent tests (5 users)...')
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i in range(5):
                for backend_name, backend_url in backends:
                    future = executor.submit(test_endpoint, backend_name, backend_url, '10000', f'concurrent_{i}')
                    futures.append(future)
            
            concurrent_results = []
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                concurrent_results.append(result)
                all_results.append(result)
        
        # Calculate statistics
        successful_results = [r for r in all_results if r['success']]
        
        if successful_results:
            response_times = [r['response_time'] for r in successful_results]
            trace_counts = [r['trace_count'] for r in successful_results if 'trace_count' in r]
            
            print('\\nğŸ“Š Performance Statistics:')
            print('=' * 50)
            print(f'Total tests: {len(all_results)}')
            print(f'Successful: {len(successful_results)}')
            print(f'Success rate: {len(successful_results)/len(all_results)*100:.1f}%')
            print(f'Average response time: {statistics.mean(response_times):.2f}s')
            print(f'Median response time: {statistics.median(response_times):.2f}s')
            print(f'Min response time: {min(response_times):.2f}s')
            print(f'Max response time: {max(response_times):.2f}s')
            
            if trace_counts:
                print(f'Average traces per query: {statistics.mean(trace_counts):.1f}')
            
            # Performance thresholds
            avg_time = statistics.mean(response_times)
            success_rate = len(successful_results)/len(all_results)
            
            print('\\nğŸ¯ Performance Evaluation:')
            if avg_time < 30 and success_rate > 0.95:
                print('âœ… EXCELLENT - System performing optimally')
            elif avg_time < 45 and success_rate > 0.90:
                print('âœ… GOOD - System performing well')
            elif avg_time < 60 and success_rate > 0.80:
                print('âš ï¸ ACCEPTABLE - System needs monitoring')
            else:
                print('âŒ POOR - System needs attention')
                exit(1)
        else:
            print('âŒ All tests failed!')
            exit(1)
        "
        
    - name: ğŸ“Š Generate Performance Report
      if: always()
      run: |
        echo "# ğŸ“Š Performance Test Report" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Test Date**: $(date)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ¯ Test Coverage" >> $GITHUB_STEP_SUMMARY
        echo "- Sequential tests for both backends" >> $GITHUB_STEP_SUMMARY
        echo "- Concurrent load tests (5 users)" >> $GITHUB_STEP_SUMMARY
        echo "- Multiple query types tested" >> $GITHUB_STEP_SUMMARY
        echo "- Response time and trace analysis" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ğŸ”— Tested Endpoints" >> $GITHUB_STEP_SUMMARY
        echo "- **Node.js**: ${{ env.NODEJS_BACKEND_URL }}" >> $GITHUB_STEP_SUMMARY
        echo "- **N8N**: ${{ env.N8N_BACKEND_URL }}" >> $GITHUB_STEP_SUMMARY
